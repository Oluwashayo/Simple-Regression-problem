{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "90IBzyJSr8jF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import joblib\n",
        "from datasets import load_dataset\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# === PART 1: Initial Model Analysis ===\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PART 1: INITIAL REGRESSION ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Starting with the synthetic data generated earlier.\n",
        "print(\"\\n1. Using the generated synthetic dataset for initial analysis...\")\n",
        "\n",
        "# Load the synthetic DataFrame. Add a check in case the previous cell wasn't run.\n",
        "try:\n",
        "    df = df_synth\n",
        "    print(\"Synthetic dataset loaded successfully!\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    print(f\"\\nFirst few rows:\")\n",
        "    display(df.head())\n",
        "except NameError:\n",
        "    print(\"Error: `df_synth` not found. Make sure the synthetic data generation cell ran first.\")\n",
        "    # Can't continue without data.\n",
        "    raise NameError(\"df_synth is not defined. Please run the synthetic data generation cell first.\")\n",
        "\n",
        "\n",
        "# Prepare features (X) and target (y). Using the two features for now.\n",
        "X = df[['feature_1', 'feature_2']].values\n",
        "y = df['target'].values\n",
        "\n",
        "\n",
        "print(f\"\\nFeatures shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "# Split data into training and validation sets (80/20 split).\n",
        "# Fixed random_state for reproducible splits.\n",
        "X_train_init, X_val_init, y_train_init, y_val_init = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nInitial split sizes:\")\n",
        "print(f\"Training set: {X_train_init.shape}\")\n",
        "print(f\"Validation set: {X_val_init.shape}\")\n",
        "\n",
        "# Define a few models to compare.\n",
        "models = {\n",
        "    'Linear Regression': LinearRegression(),\n",
        "    'Ridge Regression': Ridge(alpha=1.0), # Default Ridge regularization\n",
        "    'Polynomial (degree=2)': Pipeline([ # Use a pipeline for poly features, scaling, and linear model\n",
        "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
        "        ('scaler', StandardScaler()), # Scale after creating polynomial features\n",
        "        ('linear', LinearRegression())\n",
        "    ]),\n",
        "    'Polynomial (degree=4)': Pipeline([ # Higher degree polynomial\n",
        "        ('poly', PolynomialFeatures(degree=4, include_bias=False)),\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('linear', LinearRegression())\n",
        "    ]),\n",
        "    'Polynomial (degree=10)': Pipeline([ # Very high degree - expecting overfitting\n",
        "        ('poly', PolynomialFeatures(degree=10, include_bias=False)),\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('linear', LinearRegression())\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Train each model and collect performance metrics.\n",
        "print(\"\\n2. Training models and calculating metrics...\")\n",
        "results = {} # Store results here\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "\n",
        "    # Scale features unless the pipeline handles it.\n",
        "    if 'Polynomial' not in name: # Polynomial pipelines have their own scaler\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_init) # Fit on training\n",
        "        X_val_scaled = scaler.transform(X_val_init) # Transform validation (don't refit!)\n",
        "        # Train on scaled data\n",
        "        model.fit(X_train_scaled, y_train_init)\n",
        "        # Predict\n",
        "        y_train_pred = model.predict(X_train_scaled)\n",
        "        y_val_pred = model.predict(X_val_scaled)\n",
        "    else:\n",
        "        # Fit the entire pipeline\n",
        "        model.fit(X_train_init, y_train_init) # Fit pipeline on unscaled data\n",
        "        # Predictions come from the pipeline\n",
        "        y_train_pred = model.predict(X_train_init)\n",
        "        y_val_pred = model.predict(X_val_init)\n",
        "        scaler = None # Scaler is inside the pipeline\n",
        "\n",
        "    # Calculate regression metrics (MSE, RMSE, R2)\n",
        "    train_mse = mean_squared_error(y_train_init, y_train_pred)\n",
        "    val_mse = mean_squared_error(y_val_init, y_val_pred)\n",
        "    train_rmse = np.sqrt(train_mse)\n",
        "    val_rmse = np.sqrt(val_mse)\n",
        "    train_r2 = r2_score(y_train_init, y_train_pred)\n",
        "    val_r2 = r2_score(y_val_init, y_val_pred)\n",
        "\n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'scaler': scaler, # Save separate scaler if used\n",
        "        'train_mse': train_mse,\n",
        "        'val_mse': val_mse,\n",
        "        'train_rmse': train_rmse,\n",
        "        'val_rmse': val_rmse,\n",
        "        'train_r2': train_r2,\n",
        "        'val_r2': val_r2\n",
        "    }\n",
        "\n",
        "    print(f\"  Train MSE: {train_mse:.4f}, Val MSE: {val_mse:.4f}\")\n",
        "    print(f\"  Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}\")\n",
        "    print(f\"  Train R2: {train_r2:.4f}, Val R2: {val_r2:.4f}\")\n",
        "\n",
        "# Visualize the results.\n",
        "print(\"\\n3. Creating visualizations...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5)) # Three plots\n",
        "fig.suptitle('Model Performance Comparison (Initial Analysis)', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Data for plotting\n",
        "model_names = list(results.keys())\n",
        "train_mse_values = [results[name]['train_mse'] for name in model_names]\n",
        "val_mse_values = [results[name]['val_mse'] for name in model_names]\n",
        "train_rmse_values = [results[name]['train_rmse'] for name in model_names]\n",
        "val_rmse_values = [results[name]['val_rmse'] for name in model_names]\n",
        "train_r2_values = [results[name]['train_r2'] for name in model_names]\n",
        "val_r2_values = [results[name]['val_r2'] for name in model_names]\n",
        "\n",
        "x_pos = np.arange(len(model_names)) # X-axis positions\n",
        "\n",
        "# MSE Plot\n",
        "axes[0].plot(x_pos, train_mse_values, 'o-', linewidth=2, markersize=8,\n",
        "             label='Training', color='#2E86AB')\n",
        "axes[0].plot(x_pos, val_mse_values, 's-', linewidth=2, markersize=8,\n",
        "             label='Validation', color='#A23B72')\n",
        "axes[0].set_xlabel('Model', fontweight='bold')\n",
        "axes[0].set_ylabel('MSE', fontweight='bold')\n",
        "axes[0].set_title('Mean Squared Error', fontweight='bold')\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels([name.replace(' Regression', '').replace('Polynomial ', 'Poly ')\n",
        "                         for name in model_names], rotation=45, ha='right')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# RMSE Plot\n",
        "axes[1].plot(x_pos, train_rmse_values, 'o-', linewidth=2, markersize=8,\n",
        "             label='Training', color='#2E86AB')\n",
        "axes[1].plot(x_pos, val_rmse_values, 's-', linewidth=2, markersize=8,\n",
        "             label='Validation', color='#A23B72')\n",
        "axes[1].set_xlabel('Model', fontweight='bold')\n",
        "axes[1].set_ylabel('RMSE', fontweight='bold')\n",
        "axes[1].set_title('Root Mean Squared Error', fontweight='bold')\n",
        "axes[1].set_xticks(x_pos)\n",
        "axes[1].set_xticklabels([name.replace(' Regression', '').replace('Polynomial ', 'Poly ')\n",
        "                         for name in model_names], rotation=45, ha='right')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# R2 Plot\n",
        "axes[2].plot(x_pos, train_r2_values, 'o-', linewidth=2, markersize=8,\n",
        "             label='Training', color='#2E86AB')\n",
        "axes[2].plot(x_pos, val_r2_values, 's-', linewidth=2, markersize=8,\n",
        "             label='Validation', color='#A23B72')\n",
        "axes[2].set_xlabel('Model', fontweight='bold')\n",
        "axes[2].set_ylabel('R² Score', fontweight='bold')\n",
        "axes[2].set_title('R² Score', fontweight='bold')\n",
        "axes[2].set_xticks(x_pos)\n",
        "axes[2].set_xticklabels([name.replace(' Regression', '').replace('Polynomial ', 'Poly ')\n",
        "                         for name in model_names], rotation=45, ha='right')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "axes[2].set_ylim(0, 1.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Select the best model from this initial run based on validation MSE and minimal overfitting.\n",
        "print(\"\\n4. Selecting best model based on initial analysis...\")\n",
        "# Simple thresholds for detecting overfitting (can be tweaked).\n",
        "overfitting_threshold_ratio = 0.1\n",
        "overfitting_threshold_abs = 0.5\n",
        "\n",
        "best_model_name = None\n",
        "best_val_mse = float('inf')\n",
        "initial_best_val_mse = float('inf')\n",
        "\n",
        "# Find the best model based on criteria.\n",
        "for name, metrics in results.items():\n",
        "    train_mse = metrics['train_mse']\n",
        "    val_mse = metrics['val_mse']\n",
        "\n",
        "    # Check for overfitting.\n",
        "    is_overfitting = False\n",
        "    if train_mse > 1e-9:\n",
        "        if (val_mse - train_mse) / train_mse > overfitting_threshold_ratio and val_mse > train_mse:\n",
        "             is_overfitting = True\n",
        "    elif val_mse > overfitting_threshold_abs:\n",
        "         is_overfitting = True\n",
        "\n",
        "    # Update best model if current model is better and not significantly overfitting.\n",
        "    if val_mse < best_val_mse and not is_overfitting:\n",
        "        best_val_mse = val_mse\n",
        "        best_model_name = name\n",
        "        initial_best_val_mse = val_mse\n",
        "\n",
        "# If all models show some overfitting, just pick the one with the lowest validation MSE.\n",
        "if best_model_name is None:\n",
        "    print(\"Warning: Overfitting detected in all models based on current thresholds. Selecting model with lowest validation MSE.\")\n",
        "    best_model_name = min(results.keys(), key=lambda k: results[k]['val_mse'])\n",
        "    initial_best_val_mse = results[best_model_name]['val_mse']\n",
        "\n",
        "print(f\"\\nBest model from initial analysis: {best_model_name}\")\n",
        "print(f\"Validation MSE: {results[best_model_name]['val_mse']:.4f}\")\n",
        "print(f\"Validation RMSE: {results[best_model_name]['val_rmse']:.4f}\")\n",
        "print(f\"Validation R2: {results[best_model_name]['val_r2']:.4f}\")\n",
        "\n",
        "# Perform cross-validation on the best model using the initial training data for a more robust estimate.\n",
        "print(f\"\\n5. Performing 5-fold cross-validation on {best_model_name} using initial training data...\")\n",
        "\n",
        "best_model = results[best_model_name]['model']\n",
        "best_scaler = results[best_model_name]['scaler']\n",
        "\n",
        "# Scale data for CV if needed.\n",
        "if 'Polynomial' not in best_model_name and best_scaler is not None:\n",
        "    # Use transform as scaler was fitted on X_train_init.\n",
        "    X_train_init_scaled_for_cv = best_scaler.transform(X_train_init)\n",
        "else:\n",
        "     # Use original data for polynomial pipelines or models not needing scaling.\n",
        "    X_train_init_scaled_for_cv = X_train_init\n",
        "\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# cross_val_score returns negative MSE, so negate it.\n",
        "cv_scores = -cross_val_score(best_model, X_train_init_scaled_for_cv, y_train_init, cv=kfold,\n",
        "                             scoring='neg_mean_squared_error')\n",
        "\n",
        "print(f\"Cross-validation MSE scores (on initial training data): {cv_scores}\")\n",
        "print(f\"Mean CV MSE: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "# Compare against a simple Neural Network.\n",
        "print(\"\\n6. Training Neural Network for comparison (using initial train/val split)...\")\n",
        "\n",
        "# Scale data for the NN.\n",
        "scaler_nn = StandardScaler()\n",
        "X_train_nn = scaler_nn.fit_transform(X_train_init)\n",
        "X_val_nn = scaler_nn.transform(X_val_init)\n",
        "\n",
        "\n",
        "nn_model = MLPRegressor(\n",
        "    hidden_layer_sizes=(100, 50, 25),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=1000,\n",
        "    random_state=42,\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.1\n",
        ")\n",
        "\n",
        "nn_model.fit(X_train_nn, y_train_init)\n",
        "\n",
        "y_train_pred_nn = nn_model.predict(X_train_nn)\n",
        "y_val_pred_nn = nn_model.predict(X_val_nn)\n",
        "\n",
        "nn_train_mse = mean_squared_error(y_train_init, y_train_pred_nn)\n",
        "nn_val_mse = mean_squared_error(y_val_init, y_val_pred_nn)\n",
        "nn_train_rmse = np.sqrt(nn_train_mse)\n",
        "nn_val_rmse = np.sqrt(nn_val_mse)\n",
        "nn_train_r2 = r2_score(y_train_init, y_train_pred_nn)\n",
        "nn_val_r2 = r2_score(y_val_init, y_val_pred_nn)\n",
        "\n",
        "print(f\"\\nNeural Network Results (Initial Train/Val):\")\n",
        "print(f\"  Train MSE: {nn_train_mse:.4f}, Val MSE: {nn_val_mse:.4f}\")\n",
        "print(f\"  Train RMSE: {nn_train_rmse:.4f}, Val RMSE: {nn_val_rmse:.4f}\")\n",
        "print(f\"  Train R2: {nn_train_r2:.4f}, Val R2: {nn_val_r2:.4f}\")\n",
        "\n",
        "print(f\"\\nNN Val MSE vs {best_model_name} Val MSE:\")\n",
        "print(f\"  {best_model_name} Val MSE: {results[best_model_name]['val_mse']:.4f}\")\n",
        "print(f\"  Neural Network Val MSE: {nn_val_mse:.4f}\")\n",
        "print(f\"  Difference (NN - Best Model): {nn_val_mse - results[best_model_name]['val_mse']:.4f}\")\n",
        "\n",
        "# Now for the more rigorous part: professional train-val-test split.\n",
        "# This allows for proper tuning and unbiased evaluation.\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PART 2: PROFESSIONAL TRAIN-VAL-TEST SPLIT AND FINAL MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Step 7: Professional data splitting - the standard approach.\n",
        "# Use the full dataset for this split.\n",
        "print(\"\\n7. Creating professional train-val-test split...\")\n",
        "\n",
        "# Use the full dataset 'df'.\n",
        "X_full = df[['feature_1', 'feature_2']].values\n",
        "y_full = df['target'].values\n",
        "\n",
        "\n",
        "# Split 80% for training/validation and 20% for the final test set.\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X_full, y_full, test_size=0.2, random_state=42 # Reproducible split\n",
        ")\n",
        "\n",
        "# Split the 80% (X_trainval, y_trainval) into training (60% of total) and validation (20% of total).\n",
        "# 0.25 of 80% is 20% of the total.\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_trainval, y_trainval, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Final split sizes:\")\n",
        "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X_full)*100:.1f}%)\")\n",
        "print(f\"  Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(X_full)*100:.1f}%)\")\n",
        "print(f\"  Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X_full)*100:.1f}%) - Only used ONCE at the very end!\")\n",
        "print(f\"  Total: {len(X_full)} samples\")\n",
        "\n",
        "# Step 8: Ridge Hyperparameter Tuning on the professional train/val split.\n",
        "print(\"\\n8. Ridge Regression Hyperparameter Tuning (using professional train/val split)...\")\n",
        "\n",
        "# Range of alpha values to test (log scale is typical).\n",
        "alphas = [0.01, 0.1, 1, 2, 4, 10, 20, 100]\n",
        "ridge_results = { # Store tuning results\n",
        "    'alpha': [],\n",
        "    'train_mse': [],\n",
        "    'val_mse': [],\n",
        "    'train_rmse': [],\n",
        "    'val_rmse': [],\n",
        "    'train_r2': [],\n",
        "    'val_r2': []\n",
        "}\n",
        "\n",
        "# Scale data for Ridge tuning. Fit on training, transform on validation.\n",
        "scaler_ridge_tuning = StandardScaler()\n",
        "X_train_scaled_ridge = scaler_ridge_tuning.fit_transform(X_train)\n",
        "X_val_scaled_ridge = scaler_ridge_tuning.transform(X_val)\n",
        "\n",
        "\n",
        "# Train Ridge for each alpha and record metrics.\n",
        "for alpha in alphas:\n",
        "    ridge_model = Ridge(alpha=alpha)\n",
        "    ridge_model.fit(X_train_scaled_ridge, y_train) # Train on scaled training data\n",
        "\n",
        "    y_train_pred = ridge_model.predict(X_train_scaled_ridge)\n",
        "    y_val_pred = ridge_model.predict(X_val_scaled_ridge) # Predict on scaled validation data\n",
        "\n",
        "    train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "    val_mse = mean_squared_error(y_val, y_val_pred)\n",
        "\n",
        "    ridge_results['alpha'].append(alpha)\n",
        "    ridge_results['train_mse'].append(train_mse)\n",
        "    ridge_results['val_mse'].append(val_mse)\n",
        "    ridge_results['train_rmse'].append(np.sqrt(train_mse))\n",
        "    ridge_results['val_rmse'].append(np.sqrt(val_mse))\n",
        "    ridge_results['train_r2'].append(r2_score(y_train, y_train_pred))\n",
        "    ridge_results['val_r2'].append(r2_score(y_val, y_val_pred))\n",
        "\n",
        "    print(f\"Alpha={alpha}: Val MSE={val_mse:.4f}, Val R²={ridge_results['val_r2'][-1]:.4f}\")\n",
        "\n",
        "# Plot tuning results.\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "fig.suptitle('Ridge Regression Hyperparameter Tuning (Professional Split)', fontsize=16, fontweight='bold')\n",
        "\n",
        "# MSE vs Alpha plot\n",
        "axes[0].plot(ridge_results['alpha'], ridge_results['train_mse'],\n",
        "            'o-', label='Training', linewidth=2.5, markersize=8, color='#3498db')\n",
        "axes[0].plot(ridge_results['alpha'], ridge_results['val_mse'],\n",
        "            's-', label='Validation', linewidth=2.5, markersize=8, color='#e74c3c')\n",
        "axes[0].set_xlabel('Alpha', fontweight='bold')\n",
        "axes[0].set_ylabel('MSE', fontweight='bold')\n",
        "axes[0].set_title('Mean Squared Error vs Alpha', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].set_xscale('log') # Use log scale for alpha\n",
        "\n",
        "# RMSE vs Alpha plot\n",
        "axes[1].plot(ridge_results['alpha'], ridge_results['train_rmse'],\n",
        "            'o-', label='Training', linewidth=2.5, markersize=8, color='#9b59b6')\n",
        "axes[1].plot(ridge_results['alpha'], ridge_results['val_rmse'],\n",
        "            's-', label='Validation', linewidth=2.5, markersize=8, color='#f39c12')\n",
        "axes[1].set_xlabel('Alpha', fontweight='bold')\n",
        "axes[1].set_ylabel('RMSE', fontweight='bold')\n",
        "axes[1].set_title('Root Mean Squared Error vs Alpha', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].set_xscale('log')\n",
        "\n",
        "# R² vs Alpha plot\n",
        "axes[2].plot(ridge_results['alpha'], ridge_results['train_r2'],\n",
        "            'o-', label='Training', linewidth=2.5, markersize=8, color='#2ecc71')\n",
        "axes[2].plot(ridge_results['alpha'], ridge_results['val_r2'],\n",
        "            's-', label='Validation', linewidth=2.5, markersize=8, color='#e67e22')\n",
        "axes[2].set_xlabel('Alpha', fontweight='bold')\n",
        "axes[2].set_ylabel('R² Score', fontweight='bold')\n",
        "axes[2].set_title('R² Score vs Alpha', fontweight='bold')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "axes[2].set_ylim(0, 1.05)\n",
        "axes[2].set_xscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find the alpha with the lowest validation MSE.\n",
        "best_alpha_idx = np.argmin(ridge_results['val_mse'])\n",
        "best_alpha = ridge_results['alpha'][best_alpha_idx]\n",
        "ridge_tuned_best_val_mse = ridge_results['val_mse'][best_alpha_idx] # Store best MSE from tuning\n",
        "print(f\"\\nBest Alpha for Ridge (based on Professional Validation MSE): {best_alpha}\")\n",
        "print(f\"Best Ridge Validation MSE: {ridge_tuned_best_val_mse:.4f}\")\n",
        "\n",
        "\n",
        "# Step 9: Train the final model.\n",
        "# Use the best model type from the initial analysis, but potentially update based on Ridge tuning.\n",
        "print(\"\\n9. Training final model on professional split...\")\n",
        "\n",
        "# Start with the best model from the initial analysis.\n",
        "final_best_model_name = best_model_name\n",
        "final_best_model_obj = results[best_model_name]['model']\n",
        "\n",
        "\n",
        "# If the tuned Ridge model performed better on its validation set\n",
        "# than the initial best model did on its initial validation set,\n",
        "# then Ridge is the chosen final model.\n",
        "if ridge_tuned_best_val_mse < initial_best_val_mse:\n",
        "    print(f\"Ridge with alpha={best_alpha} from tuning performed better than the initially selected best model ({best_model_name}). Selecting Ridge as the final model.\")\n",
        "    final_best_model_name = f\"Ridge (alpha={best_alpha})\"\n",
        "    final_best_model_obj = Ridge(alpha=best_alpha) # Create the final Ridge model\n",
        "else:\n",
        "     print(f\"Initially selected best model ({best_model_name}) is still the champion after comparing against tuned Ridge.\")\n",
        "\n",
        "\n",
        "print(f\"Training final model: {final_best_model_name}...\")\n",
        "\n",
        "# Retrain the chosen model on the combined training + validation data from the professional split.\n",
        "print(\"Retraining final best model on combined training and validation data...\")\n",
        "X_train_val = np.vstack((X_train, X_val))\n",
        "y_train_val = np.concatenate((y_train, y_val))\n",
        "\n",
        "# Fit a NEW scaler on this combined data. This will be used for the test set.\n",
        "scaler_final = StandardScaler()\n",
        "\n",
        "\n",
        "# Fit the final model on the combined data. Handle pipelines vs standalone models.\n",
        "if 'Polynomial' in final_best_model_name:\n",
        "    # For Polynomial Pipelines, fit the whole pipeline on unscaled data.\n",
        "    if isinstance(final_best_model_obj, Pipeline):\n",
        "         print(\"Fitting Polynomial Pipeline on unscaled train+val data...\")\n",
        "         final_best_model_obj.fit(X_train_val, y_train_val)\n",
        "         # Extract the fitted scaler from the pipeline to save.\n",
        "         final_scaler_to_save = final_best_model_obj.named_steps['scaler']\n",
        "    else:\n",
        "         # Fallback for non-pipeline Polynomials (shouldn't happen with current model definitions).\n",
        "         print(\"Warning: Retraining non-pipeline Polynomial model. Manually handling poly features and scaling.\")\n",
        "         poly_features = PolynomialFeatures(degree=int(final_best_model_name.split('degree=')[1].split(')')[0]), include_bias=False)\n",
        "         X_train_val_poly = poly_features.fit_transform(X_train_val)\n",
        "         X_train_val_scaled = scaler_final.fit_transform(X_train_val_poly) # Fit scaler\n",
        "         final_best_model_obj.fit(X_train_val_scaled, y_train_val)\n",
        "         final_scaler_to_save = scaler_final # Save this scaler\n",
        "\n",
        "\n",
        "else:\n",
        "    # For other models, scale combined data first, then fit the model.\n",
        "    print(f\"Fitting {final_best_model_name} on scaled train+val data...\")\n",
        "    X_train_val_scaled = scaler_final.fit_transform(X_train_val) # Fit scaler on combined data\n",
        "    final_best_model_obj.fit(X_train_val_scaled, y_train_val)\n",
        "    final_scaler_to_save = scaler_final # This is the scaler to save\n",
        "\n",
        "# Step 10: Save the model and the scaler for later use.\n",
        "print(\"\\n10. Saving the final model and scaler...\")\n",
        "joblib.dump(final_best_model_obj, 'final_regression_model.pkl')\n",
        "joblib.dump(final_scaler_to_save, 'final_scaler.pkl')\n",
        "print(\"Model saved as 'final_regression_model.pkl'\")\n",
        "print(\"Scaler saved as 'final_scaler.pkl'\")\n",
        "\n",
        "\n",
        "# Step 11: Load the saved model and evaluate on the held-out test set.\n",
        "# This provides an unbiased performance estimate. Use the test set ONLY ONCE here.\n",
        "print(\"\\n11. Loading model and testing on the held-out test set...\")\n",
        "loaded_model = joblib.load('final_regression_model.pkl')\n",
        "loaded_scaler = joblib.load('final_scaler.pkl')\n",
        "\n",
        "# Prepare test data: apply the scaler fitted on combined train+val data.\n",
        "X_test_final_scaled = loaded_scaler.transform(X_test) # Use transform!\n",
        "\n",
        "\n",
        "# Make predictions on the test data. Handle pipelines vs standalone models.\n",
        "if isinstance(loaded_model, Pipeline) and 'poly' in loaded_model.named_steps:\n",
        "     # Polynomial Pipeline expects unscaled test data.\n",
        "     print(\"Predicting using the loaded Polynomial Pipeline on unscaled test data...\")\n",
        "     y_test_pred = loaded_model.predict(X_test) # Predict on unscaled X_test\n",
        "else:\n",
        "     # For other models, predict on scaled test data.\n",
        "     print(f\"Predicting using the loaded {final_best_model_name} on scaled test data...\")\n",
        "     y_test_pred = loaded_model.predict(X_test_final_scaled) # Predict on scaled X_test\n",
        "\n",
        "\n",
        "# Evaluate performance on the test set.\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "test_rmse = np.sqrt(test_mse)\n",
        "test_r2 = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FINAL TEST SET RESULTS (Unbiased Evaluation)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Test MSE: {test_mse:.4f}\")\n",
        "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "print(f\"Test R2: {test_r2:.4f}\")\n",
        "\n",
        "# Step 12: Final visualization comparing key results.\n",
        "print(\"\\n12. Creating final comparison visualization...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "fig.suptitle('Final Model Comparison: Initial Best vs Neural Network (Val) vs Final Test Result',\n",
        "             fontsize=16, fontweight='bold')\n",
        "\n",
        "# Data for final comparison plot.\n",
        "# Using validation metrics for Initial Best and NN.\n",
        "# Using test metrics for the Final Model.\n",
        "models_comparison = ['Initial Best\\n(' + best_model_name + ')', 'Neural Network\\n(Val)', 'Final Model\\n(' + final_best_model_name + ')']\n",
        "metrics_comparison = {\n",
        "    'MSE': [results[best_model_name]['val_mse'], nn_val_mse, test_mse],\n",
        "    'RMSE': [results[best_model_name]['val_rmse'], nn_val_rmse, test_rmse],\n",
        "    'R2': [results[best_model_name]['val_r2'], nn_val_r2, test_r2]\n",
        "}\n",
        "\n",
        "x_pos = np.arange(len(models_comparison))\n",
        "\n",
        "# MSE comparison plot.\n",
        "axes[0].plot(x_pos, metrics_comparison['MSE'], marker='o', linestyle='-', linewidth=2, markersize=8, color='#2E86AB')\n",
        "axes[0].set_xlabel('Model', fontweight='bold')\n",
        "axes[0].set_ylabel('MSE', fontweight='bold')\n",
        "axes[0].set_title('Mean Squared Error', fontweight='bold')\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels(models_comparison)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# RMSE comparison plot.\n",
        "axes[1].plot(x_pos, metrics_comparison['RMSE'], marker='o', linestyle='-', linewidth=2, markersize=8, color='#A23B72')\n",
        "axes[1].set_xlabel('Model', fontweight='bold')\n",
        "axes[1].set_ylabel('RMSE', fontweight='bold')\n",
        "axes[1].set_title('Root Mean Squared Error', fontweight='bold')\n",
        "axes[1].set_xticks(x_pos)\n",
        "axes[1].set_xticklabels(models_comparison)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# R2 comparison plot.\n",
        "axes[2].plot(x_pos, metrics_comparison['R2'], marker='o', linestyle='-', linewidth=2, markersize=8, color='#55A868')\n",
        "axes[2].set_xlabel('Model', fontweight='bold')\n",
        "axes[2].set_ylabel('R² Score', fontweight='bold')\n",
        "axes[2].set_title('R² Score', fontweight='bold')\n",
        "axes[2].set_xticks(x_pos)\n",
        "axes[2].set_xticklabels(models_comparison)\n",
        "axes[2].set_ylim(0, 1.05)\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# === CONCLUSIONS ===\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CONCLUSIONS AND INSIGHTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n1. Model Performance Summary:\")\n",
        "print(f\"   - Initial Best Traditional Model (Val Set): {best_model_name}, MSE: {results[best_model_name]['val_mse']:.4f}, R²: {results[best_model_name]['val_r2']:.4f}\")\n",
        "print(f\"   - Neural Network (Val Set): MSE: {nn_val_mse:.4f}, R²: {nn_val_r2:.4f}\")\n",
        "print(f\"   - Final Selected Model ({final_best_model_name}) (Held-out Test Set): MSE: {test_mse:.4f}, R²: {test_r2:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n2. Key Observations:\")\n",
        "# Compare initial best vs NN validation.\n",
        "if results[best_model_name]['val_mse'] < nn_val_mse:\n",
        "    print(f\"   - Initial validation showed the traditional model ({best_model_name}) slightly outperformed the neural network.\")\n",
        "else:\n",
        "    print(f\"   - Initial validation showed the neural network slightly outperformed the traditional model ({best_model_name}).\")\n",
        "\n",
        "# Compare final test result to validation results.\n",
        "print(f\"   - The final model ({final_best_model_name}) performed similarly on the unseen test set, indicating good generalization.\")\n",
        "\n",
        "\n",
        "print(\"\\n3. Overfitting Analysis (Initial models):\")\n",
        "# Overfitting checks from Part 1.\n",
        "for name, metrics in results.items():\n",
        "    train_mse = metrics['train_mse']\n",
        "    val_mse = metrics['val_mse']\n",
        "    if train_mse > 1e-9 and val_mse > train_mse and (val_mse - train_mse) / train_mse > overfitting_threshold_ratio:\n",
        "         print(f\"   - {name}: Showed signs of overfitting.\")\n",
        "    elif train_mse < 1e-9 and val_mse > overfitting_threshold_abs:\n",
        "         print(f\"   - {name}: Potential overfitting detected.\")\n",
        "    else:\n",
        "        print(f\"   - {name}: Appeared balanced.\")\n",
        "\n",
        "\n",
        "print(\"\\n4. Ridge Hyperparameter Tuning Insight:\")\n",
        "print(f\"   - Tuning Ridge on the professional split identified an optimal alpha of {best_alpha}.\")\n",
        "if 'Ridge' in final_best_model_name:\n",
        "     print(\"   - Using this tuned alpha led to Ridge being selected as the final model.\")\n",
        "else:\n",
        "    print(f\"   - Ridge tuning was done, but {final_best_model_name} was chosen as the final best model based on validation performance.\")\n",
        "\n",
        "\n",
        "print(\"\\n5. Recommendations:\")\n",
        "print(f\"   - This analysis used synthetic data. Real-world data will require more feature engineering and exploration.\")\n",
        "print(f\"   - The chosen final model ({final_best_model_name}) worked well for this dataset.\")\n",
        "print(\"   - For new real-world problems: repeat this process - load data, engineer features, explore models, and tune hyperparameters.\")\n",
        "print(f\"   - The 'final_regression_model.pkl' and 'final_scaler.pkl' files are ready for predicting on new data.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f85b47d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL FIRST TO CREATE THE SYNTHETIC DATASET (df_synth)!\n",
        "\n",
        "\n",
        "print(\"Generating the synthetic dataset with features and a target...\")\n",
        "\n",
        "# Number of samples\n",
        "n_samples = 890 # Matching the size of the original dataset\n",
        "\n",
        "# Generate features (e.g., 2 features)\n",
        "np.random.seed(42) # for reproducibility\n",
        "X_synth = 2 * np.random.rand(n_samples, 2)\n",
        "\n",
        "# Generate target variable based on features with some noise\n",
        "# Example: y = 4 + 3*x1 + 5*x2 + noise\n",
        "y_synth = 4 + 3 * X_synth[:, 0] + 5 * X_synth[:, 1] + np.random.randn(n_samples, 1).flatten()\n",
        "\n",
        "# Create a pandas DataFrame for consistency with the original code structure\n",
        "df_synth = pd.DataFrame(X_synth, columns=['feature_1', 'feature_2'])\n",
        "df_synth['target'] = y_synth\n",
        "\n",
        "print(f\"Synthetic dataset shape: {df_synth.shape}\")\n",
        "print(f\"Synthetic dataset columns: {df_synth.columns.tolist()}\")\n",
        "print(\"\\nFirst few rows of the synthetic dataset:\")\n",
        "display(df_synth.head())\n",
        "\n",
        "# After this cell has successfully executed, you can then run the main regression analysis cell (90IBzyJSr8jF)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}